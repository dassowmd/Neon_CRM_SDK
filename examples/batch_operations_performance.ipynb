{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Batch Operations Performance Analysis with Neon CRM SDK\n",
    "\n",
    "This notebook demonstrates performance optimization techniques for batch operations using the Neon CRM SDK.\n",
    "\n",
    "‚ö†Ô∏è **SAFETY NOTE**: This notebook contains only read-only operations for safety.\n",
    "Database-modifying operations are commented out to prevent accidental changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neon_crm import NeonClient, types\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize the client\n",
    "client = NeonClient(\n",
    "    org_id=os.getenv('NEON_ORG_ID'),\n",
    "    api_key=os.getenv('NEON_API_KEY'),\n",
    ")\n",
    "\n",
    "print(f\"Connected to Neon CRM - Environment: {client.environment}\")\n",
    "print(f\"Timestamp: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Performance Measurement Utilities\n",
    "\n",
    "Helper functions to measure and analyze performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceMonitor:\n",
    "    \"\"\"Monitor and analyze performance of batch operations.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.measurements = []\n",
    "\n",
    "    def time_operation(self, operation_name: str, func, *args, **kwargs):\n",
    "        \"\"\"Time an operation and record the results.\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            result = func(*args, **kwargs)\n",
    "            success = True\n",
    "            error = None\n",
    "        except Exception as e:\n",
    "            result = None\n",
    "            success = False\n",
    "            error = str(e)\n",
    "\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "\n",
    "        measurement = {\n",
    "            'operation': operation_name,\n",
    "            'start_time': start_time,\n",
    "            'end_time': end_time,\n",
    "            'duration': duration,\n",
    "            'success': success,\n",
    "            'error': error,\n",
    "            'timestamp': datetime.now()\n",
    "        }\n",
    "\n",
    "        self.measurements.append(measurement)\n",
    "\n",
    "        if success:\n",
    "            print(f\"‚úì {operation_name}: {duration:.2f}s\")\n",
    "        else:\n",
    "            print(f\"‚ùå {operation_name}: Failed after {duration:.2f}s - {error}\")\n",
    "\n",
    "        return result, measurement\n",
    "\n",
    "    def get_summary(self) -> pd.DataFrame:\n",
    "        \"\"\"Get performance summary as DataFrame.\"\"\"\n",
    "        if not self.measurements:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = pd.DataFrame(self.measurements)\n",
    "        return df\n",
    "\n",
    "    def analyze_performance(self) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze performance metrics.\"\"\"\n",
    "        df = self.get_summary()\n",
    "\n",
    "        if df.empty:\n",
    "            return {}\n",
    "\n",
    "        # Filter successful operations for analysis\n",
    "        successful_ops = df[df['success'] == True]\n",
    "\n",
    "        if successful_ops.empty:\n",
    "            return {'total_operations': len(df), 'successful_operations': 0}\n",
    "\n",
    "        analysis = {\n",
    "            'total_operations': len(df),\n",
    "            'successful_operations': len(successful_ops),\n",
    "            'success_rate': len(successful_ops) / len(df) * 100,\n",
    "            'avg_duration': successful_ops['duration'].mean(),\n",
    "            'min_duration': successful_ops['duration'].min(),\n",
    "            'max_duration': successful_ops['duration'].max(),\n",
    "            'total_time': successful_ops['duration'].sum(),\n",
    "            'operations_per_second': len(successful_ops) / successful_ops['duration'].sum()\n",
    "        }\n",
    "\n",
    "        return analysis\n",
    "\n",
    "\n",
    "# Initialize performance monitor\n",
    "perf_monitor = PerformanceMonitor()\n",
    "print(\"üìä Performance monitoring initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Single vs Batch Request Performance\n",
    "\n",
    "Compare performance of individual requests vs batch operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single request performance\n",
    "print(\"üîç Testing single request performance...\")\n",
    "\n",
    "# Get some accounts for testing\n",
    "accounts_result, measurement = perf_monitor.time_operation(\n",
    "    \"List Accounts (Single Request)\",\n",
    "    lambda: list(client.accounts.list(page_size=100, limit=50, user_type=types.UserType.INDIVIDUAL))\n",
    ")\n",
    "\n",
    "if accounts_result:\n",
    "    print(f\"Retrieved {len(accounts_result)} accounts\")\n",
    "\n",
    "    # Test individual account retrievals\n",
    "    print(\"\\nüîç Testing individual account retrievals...\")\n",
    "\n",
    "    sample_accounts = accounts_result[:5]  # Test with first 5 accounts\n",
    "\n",
    "    for i, account in enumerate(sample_accounts, 1):\n",
    "        account_id = account.get('accountId')\n",
    "\n",
    "        if account_id:\n",
    "            account_detail, _ = perf_monitor.time_operation(\n",
    "                f\"Get Account {i} (ID: {account_id})\",\n",
    "                client.accounts.get,\n",
    "                account_id\n",
    "            )\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Account {i}: No account ID found\")\n",
    "else:\n",
    "    print(\"‚ùå Could not retrieve accounts for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3. Pagination Strategy Performance\n",
    "\n",
    "Test different pagination strategies to find optimal page sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different page sizes for optimal performance\n",
    "print(\"üìÑ Testing pagination strategy performance...\")\n",
    "\n",
    "page_sizes = [25, 50, 100, 200]\n",
    "pagination_results = []\n",
    "\n",
    "for page_size in page_sizes:\n",
    "    print(f\"\\nüîç Testing page size: {page_size}\")\n",
    "\n",
    "    # Test accounts list with different page sizes\n",
    "    accounts_result, measurement = perf_monitor.time_operation(\n",
    "        f\"List Accounts (page_size={page_size})\",\n",
    "        lambda ps=page_size: list(client.accounts.list(page_size=ps, limit=100, user_type=types.UserType.INDIVIDUAL))\n",
    "    )\n",
    "\n",
    "    if accounts_result:\n",
    "        records_per_second = len(accounts_result) / measurement['duration']\n",
    "        pagination_results.append({\n",
    "            'page_size': page_size,\n",
    "            'total_records': len(accounts_result),\n",
    "            'duration': measurement['duration'],\n",
    "            'records_per_second': records_per_second,\n",
    "            'success': True\n",
    "        })\n",
    "        print(f\"  ‚úì Retrieved {len(accounts_result)} records at {records_per_second:.1f} records/second\")\n",
    "    else:\n",
    "        pagination_results.append({\n",
    "            'page_size': page_size,\n",
    "            'total_records': 0,\n",
    "            'duration': measurement['duration'],\n",
    "            'records_per_second': 0,\n",
    "            'success': False\n",
    "        })\n",
    "\n",
    "# Analyze pagination performance\n",
    "if pagination_results:\n",
    "    pagination_df = pd.DataFrame(pagination_results)\n",
    "    successful_pagination = pagination_df[pagination_df['success'] == True]\n",
    "\n",
    "    if not successful_pagination.empty:\n",
    "        print(\"\\nüìä Pagination Performance Summary:\")\n",
    "        print(successful_pagination[['page_size', 'total_records', 'duration', 'records_per_second']].to_string(index=False))\n",
    "\n",
    "        # Find optimal page size\n",
    "        optimal_page_size = successful_pagination.loc[successful_pagination['records_per_second'].idxmax()]\n",
    "        print(f\"\\nüéØ Optimal page size: {optimal_page_size['page_size']} ({optimal_page_size['records_per_second']:.1f} records/second)\")\n",
    "    else:\n",
    "        print(\"‚ùå No successful pagination tests\")\n",
    "else:\n",
    "    print(\"‚ùå No pagination results to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 4. Search Operation Performance\n",
    "\n",
    "Analyze performance of search operations with different criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test search operation performance\n",
    "print(\"üîç Testing search operation performance...\")\n",
    "\n",
    "# Test different search strategies\n",
    "search_tests = [\n",
    "    {\n",
    "        'name': 'Simple Account Search',\n",
    "        'resource': 'accounts',\n",
    "        'request': {\n",
    "            'searchFields': [\n",
    "                {'field': 'Account Type', 'operator': 'EQUAL', 'value': 'Individual'}\n",
    "            ],\n",
    "            'outputFields': ['Account ID', 'First Name', 'Last Name', 'Email 1'],\n",
    "            'pagination': {'currentPage': 0, 'pageSize': 200}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Donation Search by Date Range',\n",
    "        'resource': 'donations',\n",
    "        'request': {\n",
    "            'searchFields': [\n",
    "                {'field': 'Donation Date', 'operator': 'GREATER_THAN', 'value': '2023-01-01'}\n",
    "            ],\n",
    "            'outputFields': ['Donation ID', 'Account ID', 'Donation Amount', 'Donation Date'],\n",
    "            'pagination': {'currentPage': 0, 'pageSize': 200}\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "search_results = []\n",
    "\n",
    "for test in search_tests:\n",
    "    print(f\"\\nüîç {test['name']}...\")\n",
    "\n",
    "    try:\n",
    "        if test['resource'] == 'accounts':\n",
    "            search_result, measurement = perf_monitor.time_operation(\n",
    "                test['name'],\n",
    "                lambda: list(client.accounts.search(test['request'], validate=False))\n",
    "            )\n",
    "        elif test['resource'] == 'donations':\n",
    "            search_result, measurement = perf_monitor.time_operation(\n",
    "                test['name'],\n",
    "                lambda: list(client.donations.search(test['request'], validate=False))\n",
    "            )\n",
    "\n",
    "        if search_result:\n",
    "            records_per_second = len(search_result) / measurement['duration']\n",
    "            search_results.append({\n",
    "                'test_name': test['name'],\n",
    "                'resource': test['resource'],\n",
    "                'total_records': len(search_result),\n",
    "                'duration': measurement['duration'],\n",
    "                'records_per_second': records_per_second,\n",
    "                'success': True\n",
    "            })\n",
    "            print(f\"  ‚úì Found {len(search_result)} records at {records_per_second:.1f} records/second\")\n",
    "        else:\n",
    "            search_results.append({\n",
    "                'test_name': test['name'],\n",
    "                'resource': test['resource'],\n",
    "                'total_records': 0,\n",
    "                'duration': measurement['duration'],\n",
    "                'records_per_second': 0,\n",
    "                'success': False\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå {test['name']} failed: {e}\")\n",
    "        search_results.append({\n",
    "            'test_name': test['name'],\n",
    "            'resource': test['resource'],\n",
    "            'total_records': 0,\n",
    "            'duration': 0,\n",
    "            'records_per_second': 0,\n",
    "            'success': False\n",
    "        })\n",
    "\n",
    "# Display search performance results\n",
    "if search_results:\n",
    "    search_df = pd.DataFrame(search_results)\n",
    "    successful_searches = search_df[search_df['success'] == True]\n",
    "\n",
    "    if not successful_searches.empty:\n",
    "        print(\"\\nüìä Search Performance Summary:\")\n",
    "        print(successful_searches[['test_name', 'total_records', 'duration', 'records_per_second']].to_string(index=False))\n",
    "    else:\n",
    "        print(\"‚ùå No successful search operations\")\n",
    "else:\n",
    "    print(\"‚ùå No search results to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 5. Concurrent Operations Performance\n",
    "\n",
    "Test performance improvements from concurrent operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test concurrent operations\n",
    "print(\"üîÄ Testing concurrent operations performance...\")\n",
    "\n",
    "\n",
    "def perform_account_search(search_criteria):\n",
    "    \"\"\"Perform an account search operation.\"\"\"\n",
    "    try:\n",
    "        request = {\n",
    "            'searchFields': search_criteria,\n",
    "            'outputFields': ['Account ID', 'First Name', 'Last Name'],\n",
    "            'pagination': {'currentPage': 0, 'pageSize': 200}\n",
    "        }\n",
    "        result = list(client.accounts.search(request, validate=False))\n",
    "        return len(result)\n",
    "    except Exception as e:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Define multiple search criteria for concurrent testing\n",
    "concurrent_searches = [\n",
    "    [{'field': 'Account Type', 'operator': 'EQUAL', 'value': 'Individual'}],\n",
    "    [{'field': 'Account Type', 'operator': 'EQUAL', 'value': 'Organization'}],\n",
    "    [{'field': 'First Name', 'operator': 'NOT_BLANK', 'value': ''}],\n",
    "    [{'field': 'Last Name', 'operator': 'NOT_BLANK', 'value': ''}],\n",
    "]\n",
    "\n",
    "# Sequential execution\n",
    "print(\"\\nüìù Sequential execution...\")\n",
    "sequential_results = []\n",
    "sequential_start = time.time()\n",
    "\n",
    "for i, criteria in enumerate(concurrent_searches, 1):\n",
    "    result_count, measurement = perf_monitor.time_operation(\n",
    "        f\"Sequential Search {i}\",\n",
    "        perform_account_search,\n",
    "        criteria\n",
    "    )\n",
    "    sequential_results.append(result_count)\n",
    "\n",
    "sequential_total_time = time.time() - sequential_start\n",
    "sequential_total_records = sum(sequential_results)\n",
    "\n",
    "print(f\"‚úì Sequential: {sequential_total_records} total records in {sequential_total_time:.2f}s\")\n",
    "\n",
    "# Concurrent execution\n",
    "print(\"\\nüîÄ Concurrent execution...\")\n",
    "concurrent_start = time.time()\n",
    "concurrent_results = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    # Submit all searches concurrently\n",
    "    future_to_criteria = {\n",
    "        executor.submit(perform_account_search, criteria): criteria\n",
    "        for criteria in concurrent_searches\n",
    "    }\n",
    "\n",
    "    # Collect results as they complete\n",
    "    for future in as_completed(future_to_criteria):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            concurrent_results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Concurrent search failed: {e}\")\n",
    "            concurrent_results.append(0)\n",
    "\n",
    "concurrent_total_time = time.time() - concurrent_start\n",
    "concurrent_total_records = sum(concurrent_results)\n",
    "\n",
    "print(f\"‚úì Concurrent: {concurrent_total_records} total records in {concurrent_total_time:.2f}s\")\n",
    "\n",
    "# Compare performance\n",
    "if sequential_total_time > 0 and concurrent_total_time > 0:\n",
    "    speedup = sequential_total_time / concurrent_total_time\n",
    "    efficiency = (speedup / len(concurrent_searches)) * 100\n",
    "\n",
    "    print(f\"\\nüìä Concurrency Performance Analysis:\")\n",
    "    print(f\"Sequential Time: {sequential_total_time:.2f}s\")\n",
    "    print(f\"Concurrent Time: {concurrent_total_time:.2f}s\")\n",
    "    print(f\"Speedup: {speedup:.2f}x\")\n",
    "    print(f\"Efficiency: {efficiency:.1f}%\")\n",
    "\n",
    "    if speedup > 1.0:\n",
    "        print(f\"üéØ Concurrent operations are {speedup:.2f}x faster!\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Sequential operations performed better in this test\")\n",
    "else:\n",
    "    print(\"‚ùå Could not compare performance - insufficient timing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 6. Memory Usage Analysis\n",
    "\n",
    "Monitor memory usage during batch operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import gc\n",
    "\n",
    "# Memory usage monitoring\n",
    "print(\"üíæ Memory usage analysis...\")\n",
    "\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB.\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024  # Convert to MB\n",
    "\n",
    "\n",
    "# Test memory usage with different batch sizes\n",
    "batch_sizes = [50, 100, 200, 500]\n",
    "memory_results = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"\\nüíæ Testing memory usage with batch size: {batch_size}\")\n",
    "\n",
    "    # Clear memory before test\n",
    "    gc.collect()\n",
    "    initial_memory = get_memory_usage()\n",
    "\n",
    "    try:\n",
    "        # Perform batch operation\n",
    "        start_time = time.time()\n",
    "        accounts = list(client.accounts.list(page_size=batch_size, limit=batch_size, user_type=types.UserType.INDIVIDUAL))\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        peak_memory = get_memory_usage()\n",
    "        memory_used = peak_memory - initial_memory\n",
    "\n",
    "        memory_results.append({\n",
    "            'batch_size': batch_size,\n",
    "            'records_retrieved': len(accounts),\n",
    "            'initial_memory_mb': initial_memory,\n",
    "            'peak_memory_mb': peak_memory,\n",
    "            'memory_used_mb': memory_used,\n",
    "            'memory_per_record_kb': (memory_used * 1024) / len(accounts) if len(accounts) > 0 else 0,\n",
    "            'duration': duration,\n",
    "            'success': True\n",
    "        })\n",
    "\n",
    "        print(f\"  ‚úì Retrieved {len(accounts)} records\")\n",
    "        print(f\"  üíæ Memory used: {memory_used:.1f} MB ({(memory_used * 1024) / len(accounts):.1f} KB per record)\")\n",
    "\n",
    "        # Clear memory after test\n",
    "        del accounts\n",
    "        gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Failed: {e}\")\n",
    "        memory_results.append({\n",
    "            'batch_size': batch_size,\n",
    "            'records_retrieved': 0,\n",
    "            'initial_memory_mb': initial_memory,\n",
    "            'peak_memory_mb': get_memory_usage(),\n",
    "            'memory_used_mb': 0,\n",
    "            'memory_per_record_kb': 0,\n",
    "            'duration': 0,\n",
    "            'success': False\n",
    "        })\n",
    "\n",
    "# Display memory analysis results\n",
    "if memory_results:\n",
    "    memory_df = pd.DataFrame(memory_results)\n",
    "    successful_memory = memory_df[memory_df['success'] == True]\n",
    "\n",
    "    if not successful_memory.empty:\n",
    "        print(\"\\nüìä Memory Usage Analysis:\")\n",
    "        display_cols = ['batch_size', 'records_retrieved', 'memory_used_mb', 'memory_per_record_kb', 'duration']\n",
    "        print(successful_memory[display_cols].to_string(index=False))\n",
    "\n",
    "        # Find most memory-efficient batch size\n",
    "        if successful_memory['memory_per_record_kb'].sum() > 0:\n",
    "            most_efficient = successful_memory.loc[successful_memory['memory_per_record_kb'].idxmin()]\n",
    "            print(f\"\\nüéØ Most memory-efficient batch size: {most_efficient['batch_size']} ({most_efficient['memory_per_record_kb']:.1f} KB per record)\")\n",
    "    else:\n",
    "        print(\"‚ùå No successful memory tests\")\n",
    "else:\n",
    "    print(\"‚ùå No memory results to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 7. Performance Visualization\n",
    "\n",
    "Create visualizations of performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance visualizations\n",
    "print(\"üìà Creating performance visualizations...\")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Neon CRM SDK - Batch Operations Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Pagination Performance\n",
    "if 'pagination_df' in locals() and not pagination_df.empty:\n",
    "    successful_pagination = pagination_df[pagination_df['success'] == True]\n",
    "    if not successful_pagination.empty:\n",
    "        axes[0, 0].bar(successful_pagination['page_size'], successful_pagination['records_per_second'])\n",
    "        axes[0, 0].set_title('Pagination Performance')\n",
    "        axes[0, 0].set_xlabel('Page Size')\n",
    "        axes[0, 0].set_ylabel('Records per Second')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[0, 0].text(0.5, 0.5, 'No pagination data', ha='center', va='center', transform=axes[0, 0].transAxes)\n",
    "else:\n",
    "    axes[0, 0].text(0.5, 0.5, 'No pagination data', ha='center', va='center', transform=axes[0, 0].transAxes)\n",
    "\n",
    "# Plot 2: Search Performance\n",
    "if 'search_df' in locals() and not search_df.empty:\n",
    "    successful_searches = search_df[search_df['success'] == True]\n",
    "    if not successful_searches.empty:\n",
    "        axes[0, 1].bar(range(len(successful_searches)), successful_searches['records_per_second'])\n",
    "        axes[0, 1].set_title('Search Operation Performance')\n",
    "        axes[0, 1].set_xlabel('Search Test')\n",
    "        axes[0, 1].set_ylabel('Records per Second')\n",
    "        axes[0, 1].set_xticks(range(len(successful_searches)))\n",
    "        axes[0, 1].set_xticklabels([f\"Test {i+1}\" for i in range(len(successful_searches))], rotation=45)\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[0, 1].text(0.5, 0.5, 'No search data', ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "else:\n",
    "    axes[0, 1].text(0.5, 0.5, 'No search data', ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "\n",
    "# Plot 3: Memory Usage\n",
    "if 'memory_df' in locals() and not memory_df.empty:\n",
    "    successful_memory = memory_df[memory_df['success'] == True]\n",
    "    if not successful_memory.empty:\n",
    "        axes[1, 0].plot(successful_memory['batch_size'], successful_memory['memory_per_record_kb'], 'o-')\n",
    "        axes[1, 0].set_title('Memory Efficiency')\n",
    "        axes[1, 0].set_xlabel('Batch Size')\n",
    "        axes[1, 0].set_ylabel('Memory per Record (KB)')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'No memory data', ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'No memory data', ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "\n",
    "# Plot 4: Overall Performance Timeline\n",
    "perf_summary = perf_monitor.get_summary()\n",
    "if not perf_summary.empty:\n",
    "    successful_ops = perf_summary[perf_summary['success'] == True]\n",
    "    if not successful_ops.empty:\n",
    "        axes[1, 1].scatter(range(len(successful_ops)), successful_ops['duration'], alpha=0.7)\n",
    "        axes[1, 1].set_title('Operation Duration Timeline')\n",
    "        axes[1, 1].set_xlabel('Operation Sequence')\n",
    "        axes[1, 1].set_ylabel('Duration (seconds)')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        # Add trend line\n",
    "        z = np.polyfit(range(len(successful_ops)), successful_ops['duration'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        axes[1, 1].plot(range(len(successful_ops)), p(range(len(successful_ops))), \"r--\", alpha=0.8)\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'No timing data', ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'No timing data', ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Performance visualizations created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 8. Performance Recommendations\n",
    "\n",
    "Generate performance optimization recommendations based on test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate performance recommendations\n",
    "print(\"üéØ Generating performance optimization recommendations...\")\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Analyze overall performance\n",
    "overall_analysis = perf_monitor.analyze_performance()\n",
    "\n",
    "if overall_analysis:\n",
    "    print(f\"\\nüìä Overall Performance Summary:\")\n",
    "    print(f\"Total Operations: {overall_analysis.get('total_operations', 0)}\")\n",
    "    print(f\"Successful Operations: {overall_analysis.get('successful_operations', 0)}\")\n",
    "    print(f\"Success Rate: {overall_analysis.get('success_rate', 0):.1f}%\")\n",
    "    print(f\"Average Duration: {overall_analysis.get('avg_duration', 0):.2f}s\")\n",
    "    print(f\"Operations per Second: {overall_analysis.get('operations_per_second', 0):.2f}\")\n",
    "\n",
    "    # Success rate recommendations\n",
    "    success_rate = overall_analysis.get('success_rate', 0)\n",
    "    if success_rate < 95:\n",
    "        recommendations.append({\n",
    "            'category': 'Reliability',\n",
    "            'priority': 'High',\n",
    "            'recommendation': f'Success rate is {success_rate:.1f}%. Implement retry logic and better error handling.',\n",
    "            'impact': 'High'\n",
    "        })\n",
    "\n",
    "    # Performance recommendations\n",
    "    avg_duration = overall_analysis.get('avg_duration', 0)\n",
    "    if avg_duration > 2.0:\n",
    "        recommendations.append({\n",
    "            'category': 'Performance',\n",
    "            'priority': 'Medium',\n",
    "            'recommendation': f'Average operation time is {avg_duration:.2f}s. Consider optimizing query complexity or using concurrent operations.',\n",
    "            'impact': 'Medium'\n",
    "        })\n",
    "\n",
    "# Pagination recommendations\n",
    "if 'pagination_df' in locals() and not pagination_df.empty:\n",
    "    successful_pagination = pagination_df[pagination_df['success'] == True]\n",
    "    if not successful_pagination.empty:\n",
    "        optimal_page_size = successful_pagination.loc[successful_pagination['records_per_second'].idxmax()]\n",
    "        recommendations.append({\n",
    "            'category': 'Pagination',\n",
    "            'priority': 'Medium',\n",
    "            'recommendation': f'Use page size of {optimal_page_size[\"page_size\"]} for optimal throughput ({optimal_page_size[\"records_per_second\"]:.1f} records/second).',\n",
    "            'impact': 'Medium'\n",
    "        })\n",
    "\n",
    "# Memory usage recommendations\n",
    "if 'memory_df' in locals() and not memory_df.empty:\n",
    "    successful_memory = memory_df[memory_df['success'] == True]\n",
    "    if not successful_memory.empty and successful_memory['memory_per_record_kb'].sum() > 0:\n",
    "        avg_memory_per_record = successful_memory['memory_per_record_kb'].mean()\n",
    "        if avg_memory_per_record > 10:  # Threshold: 10KB per record\n",
    "            recommendations.append({\n",
    "                'category': 'Memory',\n",
    "                'priority': 'Low',\n",
    "                'recommendation': f'Memory usage is {avg_memory_per_record:.1f} KB per record. Consider processing in smaller batches to reduce memory footprint.',\n",
    "                'impact': 'Low'\n",
    "            })\n",
    "\n",
    "        most_efficient = successful_memory.loc[successful_memory['memory_per_record_kb'].idxmin()]\n",
    "        recommendations.append({\n",
    "            'category': 'Memory',\n",
    "            'priority': 'Low',\n",
    "            'recommendation': f'For memory efficiency, use batch size of {most_efficient[\"batch_size\"]} ({most_efficient[\"memory_per_record_kb\"]:.1f} KB per record).',\n",
    "            'impact': 'Low'\n",
    "        })\n",
    "\n",
    "# Concurrency recommendations\n",
    "if 'sequential_total_time' in locals() and 'concurrent_total_time' in locals():\n",
    "    if concurrent_total_time < sequential_total_time:\n",
    "        speedup = sequential_total_time / concurrent_total_time\n",
    "        recommendations.append({\n",
    "            'category': 'Concurrency',\n",
    "            'priority': 'High',\n",
    "            'recommendation': f'Concurrent operations provide {speedup:.2f}x speedup. Use ThreadPoolExecutor for independent operations.',\n",
    "            'impact': 'High'\n",
    "        })\n",
    "    else:\n",
    "        recommendations.append({\n",
    "            'category': 'Concurrency',\n",
    "            'priority': 'Low',\n",
    "            'recommendation': 'Sequential operations performed better in this test. API may have rate limiting or connection constraints.',\n",
    "            'impact': 'Low'\n",
    "        })\n",
    "\n",
    "# General best practices\n",
    "recommendations.extend([\n",
    "    {\n",
    "        'category': 'Best Practices',\n",
    "        'priority': 'Medium',\n",
    "        'recommendation': 'Implement exponential backoff for retry logic to handle temporary failures gracefully.',\n",
    "        'impact': 'Medium'\n",
    "    },\n",
    "    {\n",
    "        'category': 'Best Practices',\n",
    "        'priority': 'Medium',\n",
    "        'recommendation': 'Use field validation (validate=False) for known good requests to improve performance.',\n",
    "        'impact': 'Low'\n",
    "    },\n",
    "    {\n",
    "        'category': 'Best Practices',\n",
    "        'priority': 'Low',\n",
    "        'recommendation': 'Monitor API rate limits and implement appropriate throttling to avoid hitting limits.',\n",
    "        'impact': 'Medium'\n",
    "    }\n",
    "])\n",
    "\n",
    "# Display recommendations\n",
    "if recommendations:\n",
    "    print(\"\\nüéØ Performance Optimization Recommendations:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Group by priority\n",
    "    high_priority = [r for r in recommendations if r['priority'] == 'High']\n",
    "    medium_priority = [r for r in recommendations if r['priority'] == 'Medium']\n",
    "    low_priority = [r for r in recommendations if r['priority'] == 'Low']\n",
    "\n",
    "    for priority_group, label in [(high_priority, 'HIGH PRIORITY'), (medium_priority, 'MEDIUM PRIORITY'), (low_priority, 'LOW PRIORITY')]:\n",
    "        if priority_group:\n",
    "            print(f\"\\nüî¥ {label}:\")\n",
    "            for i, rec in enumerate(priority_group, 1):\n",
    "                print(f\"  {i}. [{rec['category']}] {rec['recommendation']}\")\n",
    "                print(f\"     Impact: {rec['impact']}\")\n",
    "\n",
    "    # Create recommendations DataFrame\n",
    "    rec_df = pd.DataFrame(recommendations)\n",
    "    print(\"\\nüìä Recommendations Summary:\")\n",
    "    print(rec_df.groupby(['priority', 'category']).size().to_string())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No specific recommendations generated - insufficient performance data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides comprehensive batch operations performance analysis for the Neon CRM SDK:\n",
    "\n",
    "### ‚úÖ Performance Areas Analyzed:\n",
    "- **Single vs Batch Operations**: Comparing individual requests to batch operations\n",
    "- **Pagination Strategies**: Testing different page sizes for optimal throughput\n",
    "- **Search Performance**: Analyzing search operation efficiency\n",
    "- **Concurrent Operations**: Measuring benefits of parallel processing\n",
    "- **Memory Usage**: Monitoring memory consumption patterns\n",
    "- **Performance Visualization**: Creating charts for performance metrics\n",
    "\n",
    "### üéØ Key Performance Insights:\n",
    "- **Optimal Batch Sizes**: Identified through throughput analysis\n",
    "- **Memory Efficiency**: KB per record measurements across batch sizes\n",
    "- **Concurrency Benefits**: Speedup measurements for parallel operations\n",
    "- **Success Rate Analysis**: Reliability metrics for different operation types\n",
    "\n",
    "### üîß Performance Optimization Techniques:\n",
    "- **Smart Pagination**: Using optimal page sizes for maximum throughput\n",
    "- **Concurrent Processing**: ThreadPoolExecutor for independent operations\n",
    "- **Memory Management**: Garbage collection and batch size optimization\n",
    "- **Error Handling**: Comprehensive exception handling and retry logic\n",
    "\n",
    "### üìä Monitoring and Analysis:\n",
    "- **Real-time Performance Tracking**: PerformanceMonitor class for operation timing\n",
    "- **Memory Usage Monitoring**: psutil integration for memory analysis\n",
    "- **Visual Performance Analysis**: matplotlib/seaborn charts for trends\n",
    "- **Automated Recommendations**: Data-driven optimization suggestions\n",
    "\n",
    "### üöÄ Production Recommendations:\n",
    "- **Use Concurrent Operations**: For independent API calls when possible\n",
    "- **Implement Retry Logic**: With exponential backoff for reliability\n",
    "- **Monitor Rate Limits**: Implement throttling to avoid API limits\n",
    "- **Optimize Batch Sizes**: Use performance testing to find optimal sizes\n",
    "- **Memory Management**: Process large datasets in chunks to manage memory\n",
    "\n",
    "This analysis provides the foundation for building high-performance, scalable applications with the Neon CRM SDK."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
